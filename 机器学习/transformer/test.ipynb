{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "file_names = (\n",
    "    'ted2020.tgz', # train & dev\n",
    "    'test.tgz', # test\n",
    ")\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "prefix.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = f'{prefix}/train_dev.raw'\n",
    "test_prefix = f'{prefix}/test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train_dev.raw.en <==\n",
      "Thank you so much, Chris.\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "And I say that sincerely, partly because I need that.\n",
      "Put yourselves in my position.\n",
      "I flew on Air Force Two for eight years.\n",
      "Now I have to take off my shoes or boots to get on an airplane!\n",
      "I'll tell you one quick story to illustrate what that's been like for me.\n",
      "It's a true story -- every bit of this is true.\n",
      "Soon after Tipper and I left the -- White House -- we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville.\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n",
      "==> /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train_dev.raw.zh <==\n",
      "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸。我非常感激。\n",
      "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
      "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
      "請你們設身處地為我想一想！\n",
      "我曾搭乘副總統專機八年。\n",
      "現在我卻必須脫了鞋子才能上飛機!\n",
      "讓我跟你們說一個很短的故事，你們就會明白我的日子是怎麼過的。\n",
      "這是一個真實的故事 — 徹頭徹尾都是真實的。\n",
      "在我跟我夫人蒂佩爾離開 —— 白宮 —— 後 我們從那什維爾的家開車到 東邊 50 英哩外的一個我們擁有的小農場 —\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"Full width -> half width\"\"\"\n",
    "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # Full width space: direct conversion\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "                \n",
    "def clean_s(s, lang):\n",
    "    if lang == 'en':\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace('-', '') # remove '-'\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
    "    elif lang == 'zh':\n",
    "        s = strQ2B(s) # Q2B\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('—', '')\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('_', '')\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
    "    s = ' '.join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def len_s(s, lang):\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())\n",
    "\n",
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
    "                    for s1 in l1_in_f:\n",
    "                        s1 = s1.strip()\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
      "==> /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train_dev.raw.clean.en <==\n",
      "Thank you so much , Chris .\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because I need that .\n",
      "Put yourselves in my position .\n",
      "I flew on Air Force Two for eight years .\n",
      "Now I have to take off my shoes or boots to get on an airplane !\n",
      "I'll tell you one quick story to illustrate what that's been like for me .\n",
      "It's a true story every bit of this is true .\n",
      "Soon after Tipper and I left the White House we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville .\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n",
      "==> /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train_dev.raw.clean.zh <==\n",
      "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸 。 我非常感激 。\n",
      "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
      "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
      "請你們設身處地為我想一想 !\n",
      "我曾搭乘副總統專機八年 。\n",
      "現在我卻必須脫了鞋子才能上飛機 !\n",
      "讓我跟你們說一個很短的故事 , 你們就會明白我的日子是怎麼過的 。\n",
      "這是一個真實的故事徹頭徹尾都是真實的 。\n",
      "在我跟我夫人蒂佩爾離開白宮後我們從那什維爾的家開車到東邊50英哩外的一個我們擁有的小農場\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)\n",
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.01 # 3000~4000 would suffice\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "    labels = list(range(line_num))\n",
    "    random.shuffle(labels)\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "        count = 0\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            count += 1\n",
    "        train_f.close()\n",
    "        valid_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.clean.en\n",
      "  input: /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/valid.clean.en\n",
      "  input: /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.clean.zh\n",
      "  input: /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/valid.clean.zh\n",
      "  input_format: \n",
      "  model_prefix: /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/spm8000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 1000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sen"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix=prefix/f'spm{vocab_size}',\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1,\n",
    "        model_type='unigram', # 'bpe' works as well\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "in_tag = {\n",
    "    'train': 'train.clean',\n",
    "    'valid': 'valid.clean',\n",
    "    'test': 'test.raw.clean',\n",
    "}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        out_path = prefix/f'{split}.{lang}'\n",
    "        if out_path.exists():\n",
    "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "        else:\n",
    "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
    "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
    "                    for line in in_f:\n",
    "                        line = line.strip()\n",
    "                        tok = spm_model.encode(line, out_type=str)\n",
    "                        print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./DATA/rawdata/ted2020/train.en <==\n",
      "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
      "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁opportunity ▁to ▁come ▁to ▁this ▁stage ▁ t wi ce ▁; ▁i ' m ▁extreme ly ▁gr ate ful ▁.\n",
      "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con f er ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
      "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
      "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
      "▁i ▁f le w ▁on ▁air ▁force ▁two ▁for ▁eight ▁years ▁.\n",
      "▁now ▁i ▁have ▁to ▁take ▁off ▁my ▁sh o es ▁or ▁bo ot s ▁to ▁get ▁on ▁an ▁air pla ne ▁!\n",
      "▁i ' ll ▁tell ▁you ▁one ▁ qui ck ▁story ▁to ▁i ll us tra te ▁what ▁that ' s ▁been ▁like ▁for ▁me ▁.\n",
      "▁it ' s ▁a ▁true ▁story ▁every ▁bit ▁of ▁this ▁is ▁true ▁.\n",
      "▁so on ▁after ▁ ti pp er ▁and ▁i ▁left ▁the ▁white ▁house ▁we ▁were ▁dr iv ing ▁from ▁our ▁home ▁in ▁na sh vi ll e ▁to ▁a ▁little ▁far m ▁we ▁have ▁ 50 ▁mil es ▁e as t ▁of ▁na sh vi ll e ▁.\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n",
      "==> ./DATA/rawdata/ted2020/train.zh <==\n",
      "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
      "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
      "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對 我 之前 演講 的 好 評 ▁。\n",
      "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
      "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n",
      "▁我 曾 搭 乘 副 總 統 專 機 八 年 ▁。\n",
      "▁現在 我 卻 必須 脫 了 鞋 子 才能 上 飛 機 ▁!\n",
      "▁ 讓我 跟 你們 說 一個 很 短 的故事 ▁, ▁你們 就會 明 白 我的 日 子 是 怎麼 過 的 ▁。\n",
      "▁這是一個 真實 的故事 徹 頭 徹 尾 都是 真實 的 ▁。\n",
      "▁在 我 跟 我 夫 人 蒂 佩 爾 離開 白 宮 後 我們 從 那 什 維 爾 的 家 開 車 到 東 邊 50 英 哩 外 的 一個 我們 擁有 的 小 農 場\n",
      "head: -n: No such file or directory\n",
      "head: 5: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-25 11:56:41 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='en', target_lang='zh', trainpref='/Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train', validpref='/Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/valid', testpref='/Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/test', align_suffix=None, destdir='DATA/data-bin/ted2020', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
      "2023-06-25 11:57:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
      "2023-06-25 11:58:39 | INFO | fairseq_cli.preprocess | [en] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.en: 390112 sents, 12278139 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 11:58:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
      "2023-06-25 11:58:42 | INFO | fairseq_cli.preprocess | [en] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/valid.en: 3940 sents, 124837 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 11:58:42 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
      "2023-06-25 11:58:46 | INFO | fairseq_cli.preprocess | [en] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/test.en: 4000 sents, 127497 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 11:58:46 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
      "2023-06-25 11:59:52 | INFO | fairseq_cli.preprocess | [zh] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.zh: 390112 sents, 9564717 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 11:59:52 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
      "2023-06-25 11:59:54 | INFO | fairseq_cli.preprocess | [zh] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/valid.zh: 3940 sents, 96933 tokens, 0.00722% replaced (by <unk>)\n",
      "2023-06-25 11:59:54 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
      "2023-06-25 11:59:57 | INFO | fairseq_cli.preprocess | [zh] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/test.zh: 4000 sents, 8000 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 11:59:57 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA/data-bin/ted2020\n"
     ]
    }
   ],
   "source": [
    "binpath = Path('./DATA/data-bin', dataset_name)\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess \\\n",
    "        --source-lang {src_lang}\\\n",
    "        --target-lang {tgt_lang}\\\n",
    "        --trainpref {prefix/'train'}\\\n",
    "        --validpref {prefix/'valid'}\\\n",
    "        --testpref {prefix/'test'}\\\n",
    "        --destdir {binpath}\\\n",
    "        --joined-dictionary\\\n",
    "        --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    datadir = \"./DATA/data-bin/ted2020\",\n",
    "    savedir = \"./checkpoints/rnn\",\n",
    "    source_lang = src_lang,\n",
    "    target_lang = tgt_lang,\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=2,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2.,\n",
    "    lr_warmup=4000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=15,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10, \n",
    "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"hw5.seq2seq\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cuda_env \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mCudaEnvironment()\n\u001b[1;32m      2\u001b[0m utils\u001b[39m.\u001b[39mCudaEnvironment\u001b[39m.\u001b[39mpretty_print_cuda_env_list([cuda_env])\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/fairseq/utils.py:756\u001b[0m, in \u001b[0;36mCudaEnvironment.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 756\u001b[0m     cur_device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device()\n\u001b[1;32m    757\u001b[0m     prop \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_properties(\u001b[39m\"\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cur_device))\n\u001b[1;32m    758\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m prop\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    673\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     _lazy_init()\n\u001b[1;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 12:05:09 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2023-06-25 12:05:09 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 12:05:27 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
      "2023-06-25 12:05:27 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
      "2023-06-25 12:05:27 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390112 examples\n",
      "2023-06-25 12:05:27 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
      "2023-06-25 12:05:27 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
      "2023-06-25 12:05:27 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3940 examples\n"
     ]
    }
   ],
   "source": [
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1,\n",
      " 'source': tensor([  64,    9,  853,   79,  978,   63,   31, 1318,  106,  140,  536,    6,\n",
      "          61,   20,    6,  265,  120,  652,    4,  217,   26,  141,   79,   27,\n",
      "          17,    9,  141,   24,  420,   86,    6,   16,    9, 1487,   85,    4,\n",
      "         996,    9,    5,   54,   23,   79,  106,   14,  350,  700,   11,    9,\n",
      "         247,  893,    6,   18, 1138,    9,  117,   42,   15,    6,    5,  723,\n",
      "         227,  209,   13,  383,    6,    7,    2]),\n",
      " 'target': tensor([ 114,    5,  275,  237,  115,    5,  796,  166, 2285,  289,  670, 3057,\n",
      "         190,    8, 1879, 1582,  165, 1879, 1582,  127,  211,  864, 1180,    8,\n",
      "         766,  400,   77, 3305, 1168,  205,  461, 2532, 2122, 1146,  734,  249,\n",
      "        1620, 1738, 1386,  105, 3259,  342,   36, 1186, 3875,    8, 3135, 3793,\n",
      "        1392,    2])}\n",
      "('Source: what the actor does with this information depends on its policy , '\n",
      " 'which is stored in the strengths of the connection , between the odor '\n",
      " \"detectors and the motors that power the fly's evasive actions .\")\n",
      "'Target: 「 行者 」 得到這個訊息後視乎它的政策這些政策都以關聯的強度來儲存於氣味檢測器與運動神經之間這驅動了果蠅的逃亡行為'\n"
     ]
    }
   ],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Source: <pad><pad><pad><pad> as humans , we've sort of extended the concept \"\n",
      " 'of disgust to morality .\\n'\n",
      " '<pad><pad><pad><pad> i know that for a fact . so , tell us these stories and '\n",
      " 'inspire others on the website .\\n'\n",
      " '<pad><pad><pad><pad> for me , like millions of other people around the world '\n",
      " 'today , english is an acquired language .\\n'\n",
      " '<pad><pad><pad><pad> on the fifth rung , we develop conclusions based on our '\n",
      " 'assumptions .\\n'\n",
      " '<pad><pad><pad><pad> it had a greater population of sheep than people , so '\n",
      " 'help if we needed it was nowhere to be found .\\n'\n",
      " '<pad><pad><pad><pad> these two new airplanes are the same speed as the dc8 '\n",
      " 'that was done in 1958 .\\n'\n",
      " '<pad><pad><pad><pad> i trained in classical ballet and have a background in '\n",
      " 'architecture and fashion .')\n",
      "('pre_output_tokens: 人類有點將厭惡的概念擴展至道德 '\n",
      " '。<pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '所以 , 在網站上告訴我們這些故事並啟發其他的人 。<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '對我而言 , 以及全世界幾百萬人一樣英文是第二語言<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '到了第五層 , 我們基於我們的假設得出結論<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '這裡羊比人多所以我們即使求救也沒用<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '這有件大事這就是說你根本不需要創新的區塊<pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '我學過古典芭蕾也有建築和時尚的相關背景<pad><pad><pad><pad><pad><pad><pad>')\n",
      "('Target: 人類有點將厭惡的概念擴展至道德 。<pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '所以 , 在網站上告訴我們這些故事並啟發其他的人 。<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '對我而言 , 以及全世界幾百萬人一樣英文是第二語言<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '到了第五層 , 我們基於我們的假設得出結論<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '這裡羊比人多所以我們即使求救也沒用<pad><pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '這有件大事這就是說你根本不需要創新的區塊<pad><pad><pad><pad><pad><pad><pad>\\n'\n",
      " '我學過古典芭蕾也有建築和時尚的相關背景<pad><pad><pad><pad><pad><pad><pad>')\n"
     ]
    }
   ],
   "source": [
    "seed=33\n",
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=200, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['net_input']['src_tokens'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"pre_output_tokens: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['net_input']['prev_output_tokens'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    FairseqIncrementalDecoder,\n",
    "    FairseqEncoderDecoderModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, **unused):\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # pass thru bidirectional RNN\n",
    "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        \n",
    "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # encoder_outputs: S x B x dim\n",
    "        # padding mask:  S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # project to the dimensionality of encoder_outputs\n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # compute attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
    "\n",
    "        # cancel the attention at positions corresponding to padding\n",
    "        if encoder_padding_mask is not None:\n",
    "            # leveraging broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # softmax on the dimension corresponding to source sequence\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # restore shape (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
    "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
    "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
    "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
    "        \n",
    "        self.embed_dim = args.decoder_embed_dim\n",
    "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
    "        self.num_layers = args.decoder_layers\n",
    "        \n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.attention = AttentionLayer(\n",
    "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "        ) \n",
    "        # self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],\n",
    "                self.embed_tokens.weight.shape[0],\n",
    "                bias=False,\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "        \n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        # extract the outputs from encoder\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "        # outputs:          seq_len x batch x num_directions*hidden\n",
    "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
    "        # padding_mask:     seq_len x batch\n",
    "        \n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
    "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
    "            prev_hiddens = encoder_hiddens\n",
    "        \n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "                \n",
    "        # decoder-to-encoder attention\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
    "                        \n",
    "        # pass thru unidirectional RNN\n",
    "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "        # outputs = [sequence len, batch size, hid dim]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        x = self.dropout_out_module(x)\n",
    "                \n",
    "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
    "        # we need to do an extra projection)\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        # project to vocab size\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens,\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state,\n",
    "        new_order,\n",
    "    ):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # encoder_outputs: S x B x dim\n",
    "        # padding mask:  S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # project to the dimensionality of encoder_outputs\n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # compute attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
    "\n",
    "        # cancel the attention at positions corresponding to padding\n",
    "        if encoder_padding_mask is not None:\n",
    "            # leveraging broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # softmax on the dimension corresponding to source sequence\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # restore shape (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.transformer import (\n",
    "    TransformerEncoder, \n",
    "    TransformerDecoder,\n",
    ")\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary # 源词典、目标词典\n",
    "\n",
    "    # token embeddings\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # encoder decoder\n",
    "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
    "    encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "    # encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    # decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "\n",
    "    # sequence to sequence model\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # initialization for seq2seq model is important, requires extra handling\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # weight initialization\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=256,\n",
    "    encoder_ffn_embed_dim=512,\n",
    "    encoder_layers=1,\n",
    "    decoder_embed_dim=256,\n",
    "    decoder_ffn_embed_dim=1024,\n",
    "    decoder_layers=1,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "# HINT: these patches on parameters for Transformer\n",
    "def add_transformer_args(args):\n",
    "    args.encoder_attention_heads=4\n",
    "    args.encoder_normalize_before=True\n",
    "    \n",
    "    args.decoder_attention_heads=4\n",
    "    args.decoder_normalize_before=True\n",
    "    \n",
    "    args.activation_fn=\"relu\"\n",
    "    args.max_source_positions=1024\n",
    "    args.max_target_positions=1024\n",
    "    \n",
    "    # patches on default parameters for Transformer (those not set above)\n",
    "    from fairseq.models.transformer import base_architecture\n",
    "    base_architecture(arch_args)\n",
    "\n",
    "# add_transformer_args(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa <fairseq.data.dictionary.Dictionary object at 0x7f7d0b5c2a70>\n"
     ]
    }
   ],
   "source": [
    "if config.use_wandb:\n",
    "    wandb.config.update(vars(arch_args))\n",
    "\n",
    "model = build_model(arch_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
    "        # equivalent to summing the log probs of all labels\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        # when calculating cross-entropy, add the loss of other labels\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "# generally, 0.1 is good enough\n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate(d_model, step_num, warmup_step):\n",
    "    # TODO: Change lr from constant to the equation shown above\n",
    "    lr = 0.001\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=config.lr_factor, \n",
    "    warmup=config.lr_warmup, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    scaler = GradScaler() # automatic mixed precision (amp) \n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        accum_loss = 0\n",
    "        sample_size = 0\n",
    "        # gradient accumulation: update every accum_steps samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            # mixed precision training\n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                accum_loss += loss.item()\n",
    "                # back-prop\n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # logging\n",
    "        loss_print = accum_loss/sample_size\n",
    "        stats[\"loss\"].append(loss_print)\n",
    "        progress.set_postfix(loss=loss_print)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_print,\n",
    "                \"train/grad_norm\": gnorm.item(),\n",
    "                \"train/lr\": optimizer.rate(),\n",
    "                \"train/sample_size\": sample_size,\n",
    "            })\n",
    "        \n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 15:03:29 | WARNING | fairseq.tasks.fairseq_task | 4,494 samples have invalid sizes and will be skipped, max_positions=(100, 100), first few sample ids=[88322, 88281, 88209, 76048, 90969, 90787, 88313, 88318, 88259, 90941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(8000, 256, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "epoch_itr = load_data_iterator(task, \"train\", epoch=1, max_tokens=100, num_workers=1,cached=False)\n",
    "\n",
    "src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "\n",
    "encoder_embed_tokens = nn.Embedding(len(src_dict), arch_args.encoder_embed_dim, src_dict.pad())\n",
    "decoder_embed_tokens = nn.Embedding(len(tgt_dict), arch_args.decoder_embed_dim, src_dict.pad())\n",
    "print(encoder_embed_tokens)\n",
    "# train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 8.41470985e-01,  0.00000000e+00,  8.01961795e-01, ...,\n",
       "         0.00000000e+00,  1.07460783e-04,  0.00000000e+00],\n",
       "       [ 9.09297427e-01,  0.00000000e+00,  9.58144376e-01, ...,\n",
       "         0.00000000e+00,  2.14921564e-04,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 3.79607739e-01,  0.00000000e+00,  7.45109484e-01, ...,\n",
       "         0.00000000e+00,  1.04235072e-02,  0.00000000e+00],\n",
       "       [-5.73381872e-01,  0.00000000e+00, -8.97521193e-02, ...,\n",
       "         0.00000000e+00,  1.05309621e-02,  0.00000000e+00],\n",
       "       [-9.99206834e-01,  0.00000000e+00, -8.52340887e-01, ...,\n",
       "         0.00000000e+00,  1.06384168e-02,  0.00000000e+00]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=100\n",
    "d_model=256\n",
    "position = np.arange(100)[:, np.newaxis]\n",
    "pe = np.zeros((max_len, d_model))\n",
    "div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = np.sin(position * div_term)\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FairseqIncrementDecoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRNNDecoder\u001b[39;00m(FairseqIncrementDecoder):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m():\n\u001b[1;32m      3\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_in_module \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(args\u001b[39m.\u001b[39mdropout)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FairseqIncrementDecoder' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNDecoder(FairseqIncrementDecoder):\n",
    "    def __init__():\n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim,\n",
    "            self.hidden_dim,\n",
    "            self.num_layers,\n",
    "            dropout=args.dropout,\n",
    "            batch_first=False,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionLayer(\n",
    "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "        )\n",
    "\n",
    "        # self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "\n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        \n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],\n",
    "                self.embed_tokens.weight.shape[0],\n",
    "                bias=False\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "\n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            cache_state= self.get_incremental_state(incremental_state, 'cached_state')\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            prev_hiddens = encoder_hiddens\n",
    "\n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        x = x.transpose(0,1)\n",
    "\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention((x, encoder_outputs, encoder_padding_mask)\n",
    "\n",
    "            x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "\n",
    "            x = self.dropout_out_module(x)\n",
    "\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        x = self.output_projection(x)\n",
    "\n",
    "        x = x.transpose(1,0)\n",
    "\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "\n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            src_tokens,\n",
    "            src_lengths,\n",
    "            prev_output_tokens,\n",
    "            return_all_hiddens: bool = True\n",
    "    ):\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "\n",
    "        logits, extra = src.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "\n",
    "        return logits, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.transformer import (\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "\n",
    "def build_model(args, task):\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    encoder_embed_tokens = nn.Embedding(len(Src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "\n",
    "\n",
    "    # encoder decoder\n",
    "    encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "\n",
    "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = TransformerDecoder(args, tat_dict, decoder_embed_tokens)\n",
    "\n",
    "    # sequence to sequence model\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "\n",
    "    # initialization for seq2seq model is important, requires extra handling\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "            if isinstance(module, MultiheadAttention):\n",
    "                module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.RNNBase):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name or \"bias\" in name:\n",
    "                        param.data.uniform_(-0.1, 0.1)\n",
    "            if isinstance(module, nn.RNNBase):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight\" in name or \"bias\" in name:\n",
    "                        param.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        module.apply(init_params)\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=256\n",
    "    encoder_ffn_embed_dim=512\n",
    "    encoder_layers=1\n",
    "    decoder_embed_dim=256\n",
    "    decoder_layers=1\n",
    "    share_decoder_input_output_embed=True\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "def add_transformer_args(args):\n",
    "    args.encoder_attention_heads=4\n",
    "    args.encoder_normalize_before=True\n",
    "\n",
    "    args.decoder_attention_heads=4\n",
    "    args.decoder_normalize_before=True\n",
    "\n",
    "    args.activation_fn=\"relu\"\n",
    "    args.max_source_positions=1024\n",
    "    args.max_target_positions=1024\n",
    "\n",
    "    from fairseq.models.transformer import base_architecture\n",
    "    base_architecture(arch_args)\n",
    "\n",
    "# add_transformer_args(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (386835247.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[116], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    super().\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sequence_generator = task.build_generator([model], config)\n",
    "\n",
    "def decode(toks, dictionary):\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process\n",
    "    )\n",
    "    return s if s else \"<unk>\"\n",
    "\n",
    "def inference_step(sample, model):\n",
    "    gen_out = sequence_generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    for i in range(len(gen_out)):\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_di)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "    labels = list(range(line_num))\n",
    "    random.shuffle(labels)\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "        count = 0\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            count += 1\n",
    "        train_f.close()\n",
    "        valid_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix=prefix/f'spm{vocab_size}',\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1,\n",
    "        model_type='unigram', # 'bpe' works as well\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# 1. 构建Unigram词汇表\n",
    "corpus = [\"This is the first sentence.\", \"This is the second sentence.\"]\n",
    "spm.SentencePieceTrainer.Train('--input=corpus.txt --model_prefix=unigram_model --model_type=unigram')\n",
    "\n",
    "# 2. 加载Unigram模型\n",
    "unigram_model = spm.SentencePieceProcessor()\n",
    "unigram_model.Load(\"unigram_model.model\")\n",
    "\n",
    "# 3. 构建Word Embedding模型\n",
    "sentences = [unigram_model.EncodeAsPieces(sentence) for sentence in corpus]\n",
    "embedding_model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 4. 获取单词向量\n",
    "def get_word_vector(word):\n",
    "    try:\n",
    "        vector = embedding_model.wv[word]\n",
    "    except KeyError:\n",
    "        vector = np.zeros(100)  # 如果单词不在词汇表中，返回零向量\n",
    "    return vector\n",
    "\n",
    "# 5. 示例输入单词\n",
    "input_word = \"sentence\"\n",
    "\n",
    "# 6. 获取单词向量\n",
    "word_vector = get_word_vector(input_word)\n",
    "\n",
    "# 7. 打印结果\n",
    "print(\"Word:\", input_word)\n",
    "print(\"Word Vector:\", word_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "binpath = Path('./DATA/data-bin', dataset_name)\n",
    "if binpath.exists():\n",
    "    print()\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess \\\n",
    "        --source-lang {src_lang}\\\n",
    "        --target-lang { tgt_lang}\\\n",
    "        --trainpref {prefix/'train'}\\\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (624573949.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[155], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, **unused):\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # pass thru bidirectional RNN\n",
    "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        \n",
    "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
