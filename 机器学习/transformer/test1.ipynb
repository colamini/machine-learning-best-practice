{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型构建\n",
    "import sentencepiece as spm\n",
    "\n",
    "corpus = \"corpus.txt\"\n",
    "model_prefix = \"unigram_model\"\n",
    "model_type = \"unigram\"\n",
    "vocab_size = 15\n",
    "character_coverage = 0.9995\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='train.raw.en,train.raw.zh',\n",
    "    model_prefix='colacc',\n",
    "    model_type='unigram',\n",
    "    vocab_size=22,\n",
    "    character_coverage=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型编码\n",
    "spm_model = spm.SentencePieceProcessor()\n",
    "spm_model.load(\"colacc.model\")\n",
    "\n",
    "with open('train.en', 'w') as out_f:\n",
    "    with open('train.raw.en', 'r') as in_f:\n",
    "            for line in in_f:\n",
    "                line = line.strip()\n",
    "                tok = spm_model.encode(line, out_type=str)\n",
    "                print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-29 17:46:25 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='en', target_lang='zh', trainpref='/Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train', validpref=None, testpref=None, align_suffix=None, destdir='./data-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
      "2023-06-29 17:47:01 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
      "2023-06-29 17:48:23 | INFO | fairseq_cli.preprocess | [en] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.en: 390112 sents, 12278139 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-29 17:48:23 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
      "2023-06-29 17:49:38 | INFO | fairseq_cli.preprocess | [zh] /Users/keke/workspace/machine-learning-best-practice/机器学习/transformer/DATA/rawdata/ted2020/train.zh: 390112 sents, 9564717 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-29 17:49:38 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./data-bin\n"
     ]
    }
   ],
   "source": [
    "# binarize\n",
    "import pathlib as Path\n",
    "binpath = './data-bin'\n",
    "\n",
    "!python -m fairseq_cli.preprocess \\\n",
    "    --source-lang {src_lang}\\\n",
    "    --target-lang {tgt_lang}\\\n",
    "    --trainpref {prefix/'train'}\\\n",
    "    --destdir {binpath}\\\n",
    "    --joined-dictionary\\\n",
    "    --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 17:55:45 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2023-06-29 17:55:45 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "## 构建task\n",
    "\n",
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=\"./data-bin\",\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"zh\",\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 17:59:53 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./data-bin/train.en-zh.en\n",
      "2023-06-29 17:59:53 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./data-bin/train.en-zh.zh\n",
      "2023-06-29 17:59:53 | INFO | fairseq.tasks.translation | ./data-bin train en-zh 390112 examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2,\n",
      " 'source': tensor([  19,   59,  306,    5, 1105,  491,   32, 1228,  156,   31,  138,  161,\n",
      "          43,  389,    4,   11,   19,  282,   12,  657,   88,   16,   25,   40,\n",
      "           9,  418,    5,  583,  123,    5,  283,  213,    6,   80,   64,   19,\n",
      "         179,   12,  349,    9,  231, 1960,    7,    2]),\n",
      " 'target': tensor([ 630, 3318, 3079,   62, 2568, 1431,  185,   36,  832,  175,  846, 1631,\n",
      "           8, 2005, 1231,    4,  945,  449, 1118,  678,  112,   41, 1408, 2311,\n",
      "           8,  142, 2240,   10,    2])}\n",
      "('Source: i have been blown away by this conference , and i want to thank all '\n",
      " 'of you for the many nice comments about what i had to say the other night .')\n",
      "'Target: 這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。'\n"
     ]
    }
   ],
   "source": [
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "\n",
    "sample = task.dataset(\"train\")[2]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 18:08:18 | WARNING | fairseq.tasks.fairseq_task | 252,883 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[245034, 115474, 77356, 35414, 257631, 80717, 62285, 287313, 359485, 165167]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': tensor([3965]),\n",
       " 'nsentences': 1,\n",
       " 'ntokens': 9,\n",
       " 'net_input': {'src_tokens': tensor([[  1,   1,   1,  31,  48,  13,   5,   6, 121,   6, 145, 209, 403, 292,\n",
       "             7,   2]]),\n",
       "  'src_lengths': tensor([13]),\n",
       "  'prev_output_tokens': tensor([[   2,    5, 1056,  104, 3421,  449,    8, 3448,  163,    1,    1,    1,\n",
       "              1,    1,    1,    1]])},\n",
       " 'target': tensor([[   5, 1056,  104, 3421,  449,    8, 3448,  163,    2,    1,    1,    1,\n",
       "             1,    1,    1,    1]])}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=20, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=20,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"train\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "len(src_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
